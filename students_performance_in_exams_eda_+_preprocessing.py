# -*- coding: utf-8 -*-
"""Students Performance in Exams EDA + Preprocessing

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/students-performance-in-exams-eda-preprocessing-ab47eef4-7b18-4c88-8bd9-117c0d32a7f0.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250304/auto/storage/goog4_request%26X-Goog-Date%3D20250304T045409Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D0e39279f96e3db38938790084ec913d6e9a9b9b7b6cfc48b5b0dbb3c5106fcb5b63921eb987b6f2e538b3df121cef59fd835dcc84cf74c7672711aec33489663f4f495f87bb9a920bbe8a0bda4c0264182ca9b0e40fa4aca0def7b7cd8808ee00bc257ef245752f1f74a1eeb0e7a98d5a6439bf2d64cf10182e35e62c7d3fc0379f12a2883a44ebc5c4e842625aad243510175425df3d38d835a86662abf55e9a0aba1c95b5a4cc3e1caa8cf1a39ece992ccdd0f8d62cd4a608e787616434e83033fc5ccf731df49f7223775694e06de6e0d346aa7b9ce8c946541ec2180152d3356f8c442ba99a4f3fe270499123b274da1cae9e089eb78cb1fd613e8a3e381
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
spscientist_students_performance_in_exams_path = kagglehub.dataset_download('spscientist/students-performance-in-exams')

print('Data source import complete.')

"""# Students Performance on Exams EDA


#### *Hi everyone! Thank you for taking the time to check out my notebook! I'm still learning and exploring, so there might be some mistakes or areas for improvement in my work. Iâ€™d greatly appreciate any corrections, suggestions, or feedback you can share to help me grow and refine my skills.*
#### *Thank you for your support and understanding* ðŸ˜Š
"""

import numpy as np
import pandas as pd

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

import warnings

warnings.filterwarnings("ignore")

"""## Data Loading
Let's load our data using `pandas` library.
"""

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/My Drive/DATASET/StudentsPerformance.csv')
df.head()

print(f"Dataset shape: {df.shape}")

"""## Exploratory Data Analysis with `pandas` and `matplotlib.pyplot`/`seaborn` for visualisation
#### Now, we will analyse the overall statistics of the data.
"""

df.describe()

df.info()

"""#### As we can see, 3 out of 8 attributes are numerical, while the others are categorical. Additionally, we can observe some uninformative values in the `race/ethnicity` column. Since I was unable to access the original data description, I assume that labels such as 'group A,' 'group B,' 'group C,' etc., are likely anonymized representations of different racial or ethnic groups. Therefore, we will proceed with this assumption and attempt to gain insights from the data as it is."""

df.select_dtypes(include="object").columns.tolist()

from typing import List, Tuple

# Let's define a function that prints out the summary of attributes/columns
def get_columns_summary(df: pd.DataFrame, cat_threshold: int = 20) -> Tuple[List, List]:
    cols = df.columns
    categorical_cols, quantitative_cols = [], []
    for col in cols:
        if (df[col].nunique() < cat_threshold):
            categorical_cols.append(col)
        else:
            quantitative_cols.append(col)
    print(f"Categorical columns: {categorical_cols}\n")
    print("-" * 50)
    print(f"\nQuantitative columns: {quantitative_cols}\n")
    print("-" * 50)

    for col in categorical_cols:
        print(f"\n'{col}' column contains {df[col].nunique()} unique values: {df[col].unique().tolist()}")

    print()
    print("-" * 50)
    for col in quantitative_cols:
        print(f"\n'{col}' column contains values in a range from {df.describe().loc['min', col]} to {df.describe().loc['max', col]}")

    return categorical_cols, quantitative_cols

cat_cols, quant_cols = get_columns_summary(df)

obj_cols = df[cat_cols].select_dtypes(include="object").columns.tolist()
num_cat_cols = [col for col in cat_cols if col not in obj_cols]
num_cat_cols

"""#### So, we have no numeric-looking categorical columns.

### Data Visualisation with `matplotlib.pyplot` and `seaborn`
"""

from matplotlib import pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 7))
plt.style.use("ggplot")

df.head()

from typing import List, Optional, Union

# Let's define the functions to plot the distributions of categorical/quantitative features with `sns.countplot` and `sns.histplot`
def plot_cat_feature_dist(
    data: pd.DataFrame,
    col_name: str,
    title: str,
    order: Optional[List[str]] = None,
    hue: Optional[str] = None,
    palette: Optional[str] = None,
    kind: str = "count"
):
    """
    Plots the distribution of a categorical feature with optional hue.

    Parameters:
    - data (pd.DataFrame): The dataframe containing the data.
    - col_name (str): The name of the categorical column to plot.
    - title (str): The title for the plot or group of plot.
    - order (List[str], optional): The order of the categories in the x-axis.
    - hue (str, optional): Hue column to differentiate by.
    - palette (str, optional): Palette for the plot.
    """
    if not isinstance(data, pd.DataFrame):
        raise ValueError("The `data` parameter must be a pandas DataFrame.")
    if col_name not in data.columns:
        raise ValueError("`col_name` must be valid column name in the dataframe.")

    if kind == "pie":
        data = df[col_name].value_counts()
        plt.pie(x=data, labels=data.index, autopct="%1.1f%%", startangle=90)
    else:
        sns.countplot(data=data, x=col_name, order=order, hue=hue, palette=palette)

    plt.title(f"{title} (hue: {hue})")
    plt.show()

def plot_quantitative_feature_dist(
    data: pd.DataFrame,
    col_name: str,
    title: str,
    color: Optional[str] = None,
    palette: Optional[str] = None,
    kde: bool = False,
    hue: Optional[str] = None
):
    """
    Plots the distribution of a quantitative feature with optional hue.

    Parameters:
    - data (pd.DataFrame): The dataframe containing the data.
    - col_name (str): The name of the quantitative column to plot.
    - title (str): The title for the plot.
    - color (str, optional): Color for the plot (if `hue` is not specified)
    - palette (str, optional): Palette for the plot.
    - kde (bool): Whether to show a kernel density estimate on the plot or not.
    - hue (str, optional): Hue column to differentiate by.
    """
    if not isinstance(data, pd.DataFrame):
        raise ValueError("The `data` parameter must be a pandas DataFrame.")
    if col_name not in data.columns:
        raise ValueError("`col_name` must be valid column name in the dataframe.")

    sns.histplot(data=data, x=col_name, color=color, kde=kde, hue=hue, palette=palette)
    plt.title(f"{title} (hue: {hue})")
    plt.show()

# Let's not define a function that plots the relationship between categorical features and one certain numerical(quantitative) feature
def plot_cat_num_relationship(
    data: pd.DataFrame,
    cat_cols: List[str],
    num_col: str,
    title: Optional[str],
    order: Optional[List[str]] = None,
    estimator: Optional[callable] = None,
    ci: Optional[Union[int, str]] = 95,
    palette: Optional[str] = None,
    rotation: Optional[int] = 0
):
    """
    Plots the relationship between categorical column(s) and a numerical column using a barplot(s).

    Parameters:
    - data (pd.DataFrame): The dataframe containing the data.
    - cat_cols (List[str]): The name of the categorical columns (x-axis).
    - num_col (str): The name of the numerical column (y-axis).
    - title (str): The title for the plot (if only one cat_col is provided), else: the name of numerical feature.
    - order (List[str], optional): The order of the categories on the x-axis.
    - estimator (callable, optional): The function to estimate the central tendency (e.g., mean, median).
                                      Default is `mean` if not provided.
    - ci (int or 'sd', optional): The confidence interval to draw around the estimate. Default is 95.
    - palette (str, optional): Palette for the plot.
    - rotation (int, optional): Rotation angle for x-axis labels. Default is 0.
    """
    if not isinstance(data, pd.DataFrame):
        raise ValueError("The `data` parameter must be a pandas DataFrame.")
    if not set(cat_cols).issubset(data.columns) or num_col not in data.columns:
        raise ValueError("Both `cat_cols` and `num_col` must be valid column names in the dataframe.")

    n = len(cat_cols)
    rows_num = (n + 2) // 3

    fix, axes = plt.subplots(rows_num, 3, figsize=(15, 5 * rows_num), constrained_layout=True)
    axes = axes.flatten()

    for i, col in enumerate(cat_cols):
        sns.barplot(data=data, x=col, y=num_col, order=order, estimator=estimator, ci=ci, palette=palette, ax=axes[i])
        axes[i].set_title(f"{title} vs. {col}")
        axes[i].set_xlabel(col)
        axes[i].set_ylabel(num_col)
        axes[i].tick_params(axis="x", labelrotation=rotation)

    for j in range(i + 1, len(axes)):
        axes[j].set_visible(False)

    plt.tight_layout()
    plt.show()


def plot_two_cat_cols_relationship(data: pd.DataFrame, x_col: str, hue_col: str, title: str = None, palette: Optional[str] = None, kind: Optional[str] = "count", rotation: Optional[int] = None):
    """
     Plots the relationship between two categorical columns using a `sns.catplot`.

     Parameters:
    - data (pd.DataFrame): The dataframe containing the data.
    - x_col (str): The name of the first categorical column (x-axis).
    - hue_col (str): The name of the second categorical column (hue).
    - title (str): The title for the plot.
    - palette (str, optional): Palette for the plot.
    - kind (str, optional): The kind of plot to draw, corresponds to the name of a categorical axes-level plotting function (e.g., "strip", "swarm", "box", "violin", "count", etc.)
    - rotation (int, optional): Rotation angle for x-axis labels. Defaults to 0.
    """
    if not isinstance(data, pd.DataFrame):
        raise ValueError("The `data` parameter must be a pandas DataFrame.")
    if x_col not in data.columns or hue_col not in data.columns:
        raise ValueError("Both `x_col` and `hue_col` must be valid column names in the dataframe.")

    sns.catplot(data=data, x=x_col, hue=hue_col, palette=palette, kind=kind)
    plt.title(title if title else f"{x_col} vs. {hue_col}")
    plt.xticks(rotation=rotation)
    plt.show()

# Let's visualise the distribution of gender categories
plot_cat_feature_dist(data=df, col_name="gender", title="Gender distribution", kind="pie")

plot_cat_feature_dist(data=df, col_name="race/ethnicity", title="Race/Ethnicity distribution", kind="pie")

# Let's visualise the distribution of race/ethnicity categories with gender differentiation
plot_cat_feature_dist(data=df, col_name="race/ethnicity", hue="gender", palette="autumn", order=["group A", "group B", "group C", "group D", "group E"], title="Race/Ethnicity distribution")

# Let's now check the distributions of scores (math, reading, writing)
plot_quantitative_feature_dist(data=df, col_name="math score", title="Math Score distribution", kde=True)

plot_quantitative_feature_dist(data=df, col_name="writing score", title="Writing Score distribution", kde=True)

plot_quantitative_feature_dist(data=df, col_name="reading score", title="Reading Score distribution", kde=True)

"""#### We can observe some kind of a left (negative) skewness in our scores data, which can affect the performance of different models (e.g., linear regression) and also disrupt scaling methods (e.g., Min-Max scaling). We can fix it by `log transformation`/`square root transformation`/`box-cox transformation` ad we'll do it later."""

# Let's check now the relationship between the categorical features and Math score
plot_cat_num_relationship(data=df, cat_cols=cat_cols, num_col="math score", title="math score", estimator="mean", rotation=30)

# Let's do the same with reading/writing scores
plot_cat_num_relationship(data=df, cat_cols=cat_cols, num_col="reading score", title="reading score", estimator="mean", rotation=30)

plot_cat_num_relationship(data=df, cat_cols=cat_cols, num_col="writing score", title="writing score", estimator="mean", rotation=30)

"""#### As we can see, certain attributes appear to affect student scores. For instance, the mean scores of students who have completed a `test preparation course` or have a `standard lunch` are higher compared to those who have not attended a preparation course or have `free/reduced lunch`. Additionally, students whose parents have completed some form of `higher education` tend to achieve higher scores than those whose parents have less formal education.

Let's now actually check the relationships between some categorical features:
"""

plot_two_cat_cols_relationship(data=df, x_col="parental level of education", hue_col="race/ethnicity", kind="count", rotation=30)

plot_two_cat_cols_relationship(data=df, x_col="parental level of education", hue_col="test preparation course", rotation=30, palette="autumn")

plot_two_cat_cols_relationship(data=df, x_col="parental level of education", hue_col="lunch", rotation=30, palette="spring")

plot_two_cat_cols_relationship(data=df, x_col="test preparation course", hue_col="lunch", palette="winter")

# Let's now plot the relationship between scores (e.g., "Math Score vs. Reading Score")
plt.figure(figsize=(15, 7))

plt.subplot(1, 3, 1)
sns.regplot(data=df[:200], x="math score", y="reading score", color="b")
plt.title("Math Score vs. Reading Score")

plt.subplot(1, 3, 2)
sns.regplot(data=df[:200], x="math score", y="writing score", color="g")
plt.title("Math Score vs. Writing Score")

plt.subplot(1, 3, 3)
sns.regplot(data=df[:200], x="writing score", y="reading score", color="purple")
plt.title("Writing Score vs. Reading Score")

plt.show()

"""#### We can clearly see a linear correlation between the scores, which means that students who scored high in one subject were likely to score high in all subjects.
#### In addition to this, the plots show that the variance in reading and writing scores is higher when correlated with math scores, resulting in a wider spread. In contrast, the correlation between writing and reading scores exhibits significantly lower variance, indicating a closer relationship between these two subjects.

## Data Preprocessing

### Ordinal Encoding

To facilitate our work with data, I was considering mapping the values of `gender` and `test preparation course` features (as they are binary) to the values `0` and `1`, and `parental level of education` to the values in range from `0` to `5`, but I have realised that some algorithms might interpret the values as ordinal (e.g., `0 < 1`), even though there's no such hierarchy, therefore I will only modify `test preparation course` and `parental level of education`, because these columns actually represent some kind of a hierarchy.
"""

# df["gender"] = df["gender"].map(lambda x: 0 if x == "male" else (1 if x == "female" else -1))
# df.head()

education_order = {
    'some high school': 0,
    'high school': 1,
    "associate's degree": 2,
    'some college': 3,
    "bachelor's degree": 4,
    "master's degree": 5
}

df['parental level of education'] = df['parental level of education'].map(education_order)
df.head()

df["test preparation course"] = df["test preparation course"].map(lambda x: 0 if x == "none" else (1 if x == "completed" else -1))
df.sample(10)

"""### One-Hot Encoding with `sklear.preprocessing.OneHotEncoder`
#### On other categorical features we will use `one-hot encoding` as those features have more than two categories and their values have *no ordinal relationship*.
We could use `pd.get_dummies(...)` method, but I'll proceed with Scikit-learn's `OneHotEncoder`.


With `OneHotEncoder` we are using the `drop="first"` parameter to avoid multicollinearity, which can occur while working with some ml models (e.g., linear regression, logistic regression, etc.), it also simplifies the model. But models like decision trees, random forests, or gradient boosting are not affected by multicollinearity. In these cases, you don't need to drop the first feature.
"""

from sklearn.preprocessing import OneHotEncoder

one_hot_encoder = OneHotEncoder(sparse_output=False, drop="first")

columns_to_encode = ["gender", "race/ethnicity", "lunch"]

encoded_columns = one_hot_encoder.fit_transform(df[columns_to_encode])
encoded_df = pd.DataFrame(data=encoded_columns, columns=one_hot_encoder.get_feature_names_out(columns_to_encode))

encoded_df = pd.concat([df.drop(columns=columns_to_encode), encoded_df], axis=1)
encoded_df.head()

encoded_df.describe()

"""### Min-Max Scaling Some Features

Let's use `Min-Max Scaling` normalisation technique for `parental level of education`, `math score`, `reading score` and `writing score` features to scale the data to the range [0, 1]. It will help us to work with models that are sensitive to feature magnitude, such as KNN, SVMs, etc.
"""

from sklearn.preprocessing import MinMaxScaler

cols_to_scale = ["parental level of education", "math score", "reading score", "writing score"]
scaler = MinMaxScaler(feature_range=(0, 1))  # (0, 1) is the deafult range

scaler.fit(encoded_df[cols_to_scale])

encoded_df[cols_to_scale] = scaler.transform(encoded_df[cols_to_scale])
encoded_df.head()

encoded_df.describe()

encoded_df.info()

"""We have no null values; therefore, we don't have to use any imputing techniques here.

### Now our data is ready for model training.
"""